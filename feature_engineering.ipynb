{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from collections import defaultdict\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "导入数据清洗所得到的训练集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "train_orig = pd.read_csv(\"train_orig.csv\")\n",
    "test_orig = pd.read_csv(\"test_orig.csv\")\n",
    "train_stop = pd.read_csv(\"train_stop.csv\")\n",
    "test_stop = pd.read_csv(\"test_stop.csv\")\n",
    "train_stem = pd.read_csv(\"train_stem.csv\")\n",
    "test_stem = pd.read_csv(\"test_stem.csv\")\n",
    "train_lem = pd.read_csv(\"train_lem.csv\")\n",
    "test_lem = pd.read_csv(\"test_lem.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "重新填补空值"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_orig = train_orig.fillna(\"\")\n",
    "test_orig = test_orig.fillna(\"\")\n",
    "train_stop = train_stop.fillna(\"\")\n",
    "test_stop = test_stop.fillna(\"\")\n",
    "train_stem = train_stem.fillna(\"\")\n",
    "test_stem = test_stem.fillna(\"\")\n",
    "train_lem = train_lem.fillna(\"\")\n",
    "test_lem = test_lem.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df_train = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/train_lem.csv\")\n",
    "# df_test = pd.read_csv(\"../input/quora-question-pairs-data-cleaning/test_lem.csv\")\n",
    "\n",
    "# train_corpus = pd.concat([df_train['question1'],df_train['question2']],axis=0)\n",
    "# train_corpus = train_corpus.fillna(\" \")\n",
    "# test_corpus = pd.concat([df_test['question1'],df_test['question2']],axis=0)\n",
    "# test_corpus = test_corpus.fillna(\" \")\n",
    "\n",
    "# from sklearn.feature_extraction.text import CountVectorizer\n",
    "# cntvec = CountVectorizer()\n",
    "# cntvec_mat = cntvec.fit_transform(train_corpus)\n",
    "# train_vocab = cntvec.get_feature_names()\n",
    "# cntvec2 = CountVectorizer()\n",
    "# cntvec_mat2 = cntvec2.fit_transform(test_corpus)\n",
    "# test_vocab = cntvec2.get_feature_names()\n",
    "\n",
    "# train_vocab = set(train_vocab)\n",
    "# test_vocab = set(test_vocab)\n",
    "# inter = train_vocab & test_vocab\n",
    "# oov_num = len(test_vocab) - len(inter)\n",
    "# oov_rate = oov_num / len(test_vocab)\n",
    "# print(\"Number of out of vocab words is \",oov_num)\n",
    "# print(\"The out of vocab rate is  \",oov_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "构造的特征存放在train和test中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.DataFrame(index = train_orig.index)\n",
    "test = pd.DataFrame(index = test_orig.index)\n",
    "trainlabel = train_orig[[\"is_duplicate\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 字母总数差异与差异比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "len1 = train_orig[\"question1\"].apply(len)\n",
    "len2 = train_orig[\"question2\"].apply(len)\n",
    "train[\"diff_char\"] = abs(len1 - len2)\n",
    "train[\"diff_char_rate\"] = 2 * abs(len1 - len2) / (len1 + len2)\n",
    "\n",
    "len3 = test_orig[\"question1\"].apply(len)\n",
    "len4 = test_orig[\"question2\"].apply(len)\n",
    "test[\"diff_char\"] = abs(len3 - len4)\n",
    "test[\"diff_char_rate\"] = 2 * abs(len3 - len4) / (len3 + len4)\n",
    "\n",
    "del len1, len2, len3, len4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 单词总数差异与差异比例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def words_count(text):\n",
    "    wordlist = word_tokenize(text)\n",
    "    count = len(wordlist)\n",
    "    return count\n",
    "\n",
    "count1 = train_orig[\"question1\"].apply(words_count)\n",
    "count2 = train_orig[\"question2\"].apply(words_count)\n",
    "train[\"diff_words\"] = abs(count1 - count2)\n",
    "train[\"diff_words_rate\"] = 2 * abs(count1 - count2) / (count1 + count2)\n",
    "\n",
    "count3 = test_orig[\"question1\"].apply(words_count)\n",
    "count4 = test_orig[\"question2\"].apply(words_count)\n",
    "test[\"diff_words\"] = abs(count3 - count4)\n",
    "test[\"diff_words_rate\"] = 2 * abs(count3 - count4) / (count3 + count4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 共享单词个数/比例与Jaccard相似度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shared_words_count(text1, text2):\n",
    "    wordlist1 = word_tokenize(text1)\n",
    "    wordlist2 = word_tokenize(text2)\n",
    "    wordset1 = set(wordlist1)\n",
    "    wordset2 = set(wordlist2)\n",
    "    inter = wordset1 & wordset2\n",
    "    union = wordset1 | wordset2\n",
    "    count = len(inter)\n",
    "    rate = 2 * count / (len(wordset1) + len(wordset2) + 1)  # 为了防止wordset1和wordset2同时为空，也即text1和text2都是只包含一个空格的字符串\n",
    "    jaccard = count / (len(union) + 1)\n",
    "    return pd.Series([count, rate, jaccard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "share_train_orig = train_orig[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_train_stop = train_stop[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_train_stem = train_stem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_train_lem = train_lem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_words_train = pd.concat([share_train_orig, share_train_stop, share_train_stem, share_train_lem], axis = 1)\n",
    "share_words_train.columns = [\"share_words_count_orig\", \"share_words_rate_orig\", \"jaccard_orig\", \n",
    "                             \"share_words_count_stop\", \"share_words_rate_stop\", \"jaccard_stop\", \n",
    "                             \"share_words_count_stem\", \"share_words_rate_stem\", \"jaccard_stem\",\n",
    "                             \"share_words_count_lem\", \"share_words_rate_lem\", \"jaccard_lem\"]\n",
    "train = pd.concat([train, share_words_train], axis = 1)\n",
    "\n",
    "share_test_orig = test_orig[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_test_stop = test_stop[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_test_stem = test_stem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_test_lem = test_lem[[\"question1\", \"question2\"]].apply(lambda x: shared_words_count(x[0], x[1]), axis = 1)\n",
    "share_words_test = pd.concat([share_test_orig, share_test_stop, share_test_stem, share_test_lem], axis = 1)\n",
    "share_words_test.columns = [\"share_words_count_orig\", \"share_words_rate_orig\", \"jaccard_orig\", \n",
    "                            \"share_words_count_stop\", \"share_words_rate_stop\", \"jaccard_stop\", \n",
    "                            \"share_words_count_stem\", \"share_words_rate_stem\", \"jaccard_stem\",\n",
    "                            \"share_words_count_lem\", \"share_words_rate_lem\", \"jaccard_lem\"]\n",
    "test = pd.concat([test, share_words_test], axis = 1)\n",
    "\n",
    "del share_train_orig, share_train_stop, share_train_stem, share_train_lem, share_words_train\n",
    "del share_test_orig, share_test_stop, share_test_stem, share_test_lem, share_words_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordbag = pd.concat([train_orig[\"question1\"], train_orig[\"question2\"]], axis = 0)\n",
    "tfidf = TfidfVectorizer(analyzer = \"word\", stop_words = \"english\", lowercase = True)\n",
    "tfidf.fit(wordbag)\n",
    "\n",
    "del wordbag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_q1_train = tfidf.transform(train_orig[\"question1\"])\n",
    "tfidf_q2_train = tfidf.transform(train_orig[\"question2\"])\n",
    "\n",
    "diff = tfidf_q1_train - tfidf_q2_train\n",
    "diff_tfidf_L1_train = np.sum(np.abs(diff), axis = 1)  # 统一用numpy的函数比较好\n",
    "diff_tfidf_L2_train = np.sum(diff.multiply(diff), axis = 1)\n",
    "diff_tfidf_L1_norm_train = 2 * np.array(np.sum(np.abs(diff), axis = 1)) / pd.DataFrame(count1 + count2).values\n",
    "diff_tfidf_L2_norm_train = 2 * np.array(np.sum(diff.multiply(diff), axis = 1)) / pd.DataFrame(count1 + count2).values\n",
    "# tfidf_q1_train和tfidf_q2_train，以及diff都是稀疏矩阵\n",
    "# 转换成数组再做对应元素的运算将会报错，可以用matrix对象自带的方法multiply实现\n",
    "cos_tfidf_train = np.sum(tfidf_q1_train.multiply(tfidf_q2_train), axis = 1)  # 由于词的tfidf表示是经过标准化的，所以内积即为夹角余弦值\n",
    "\n",
    "train[\"diff_tfidf_L1\"] = diff_tfidf_L1_train\n",
    "train[\"diff_tfidf_L2\"] = diff_tfidf_L2_train\n",
    "train[\"diff_tfidf_L1_norm\"] = diff_tfidf_L1_norm_train\n",
    "train[\"diff_tfidf_L2_norm\"] = diff_tfidf_L2_norm_train\n",
    "train[\"cos_tfidf\"] = cos_tfidf_train\n",
    "\n",
    "del tfidf_q1_train, tfidf_q2_train, diff, diff_tfidf_L1_train, diff_tfidf_L2_train\n",
    "del diff_tfidf_L1_norm_train, diff_tfidf_L2_norm_train, cos_tfidf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_q1_test = tfidf.transform(test_orig[\"question1\"])\n",
    "tfidf_q2_test = tfidf.transform(test_orig[\"question2\"])\n",
    "\n",
    "diff = tfidf_q1_test - tfidf_q2_test\n",
    "diff_tfidf_L1_test = np.sum(np.abs(diff), axis = 1)\n",
    "diff_tfidf_L2_test = np.sum(diff.multiply(diff), axis = 1)\n",
    "diff_tfidf_L1_norm_test = 2 * np.array(np.sum(np.abs(diff), axis = 1)) / pd.DataFrame(count3 + count4).values\n",
    "diff_tfidf_L2_norm_test = 2 * np.array(np.sum(diff.multiply(diff), axis = 1)) / pd.DataFrame(count3 + count4).values\n",
    "cos_tfidf_test = np.sum(tfidf_q1_test.multiply(tfidf_q2_test), axis = 1)\n",
    "\n",
    "test[\"diff_tfidf_L1\"] = diff_tfidf_L1_test\n",
    "test[\"diff_tfidf_L2\"] = diff_tfidf_L2_test\n",
    "test[\"diff_tfidf_L1_norm\"] = diff_tfidf_L1_norm_test\n",
    "test[\"diff_tfidf_L2_norm\"] = diff_tfidf_L2_norm_test\n",
    "test[\"cos_tfidf\"] = cos_tfidf_test\n",
    "\n",
    "del tfidf_q1_test, tfidf_q2_test, diff, diff_tfidf_L1_test, diff_tfidf_L2_test\n",
    "del diff_tfidf_L1_norm_test, diff_tfidf_L2_norm_test, cos_tfidf_test\n",
    "del count1, count2, count3, count4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题对的邻节点交集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr = pd.read_csv(\"../input/quora-question-pairs/train.csv\")\n",
    "te = pd.read_csv(\"../input/quora-question-pairs/test.csv\")\n",
    "\n",
    "ques = pd.concat([tr[[\"question1\", \"question2\"]], te[[\"question1\", \"question2\"]]], \n",
    "                 axis = 0).reset_index(drop = \"index\")\n",
    "q_dict = defaultdict(set)\n",
    "for i in range(ques.shape[0]):\n",
    "        q_dict[ques.question1[i]].add(ques.question2[i])\n",
    "        q_dict[ques.question2[i]].add(ques.question1[i])\n",
    "\n",
    "def q1_q2_intersect(row):\n",
    "    return len(set(q_dict[row['question1']]).intersection(set(q_dict[row['question2']])))\n",
    "\n",
    "train[\"q1_q2_intersect\"] = tr.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "test[\"q1_q2_intersect\"] = te.apply(q1_q2_intersect, axis=1, raw=True)\n",
    "\n",
    "del ques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 问题出现频数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q1_freq(row):\n",
    "    return(len(q_dict[row[\"question1\"]]))\n",
    "def q2_freq(row):\n",
    "    return(len(q_dict[row[\"question2\"]]))\n",
    "\n",
    "train[\"q1_freq\"] = tr.apply(q1_freq, axis=1, raw=True)\n",
    "train[\"q2_freq\"] = tr.apply(q2_freq, axis=1, raw=True)\n",
    "train[\"q1_q2_freq_average\"] = (train[\"q1_freq\"] + train[\"q2_freq\"]) / 2\n",
    "\n",
    "test[\"q1_freq\"] = te.apply(q1_freq, axis=1, raw=True)\n",
    "test[\"q2_freq\"] = te.apply(q2_freq, axis=1, raw=True)\n",
    "test[\"q1_q2_freq_average\"] = (test[\"q1_freq\"] + test[\"q2_freq\"]) / 2\n",
    "\n",
    "del tr, te, q_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 第一个单词的差异"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def same_start_word(row):\n",
    "    wordlist1 = word_tokenize(row[\"question1\"])\n",
    "    wordlist2 = word_tokenize(row[\"question2\"])\n",
    "    if wordlist1 and wordlist2:  # 为了防止question1或question2是只包含分隔符的空问题\n",
    "        return int(wordlist1[0] == wordlist2[0])\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "train[\"same_start_word\"] = train_orig.apply(same_start_word, axis = 1)\n",
    "test[\"same_start_word\"] = test_orig.apply(same_start_word, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 情感相似度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiment_analyzer = SentimentIntensityAnalyzer()\n",
    "def sentiment_analyze(row):\n",
    "    sen1 = sentiment_analyzer.polarity_scores(row[\"question1\"])\n",
    "    sen2 = sentiment_analyzer.polarity_scores(row[\"question2\"])\n",
    "    diff_neg = np.abs(sen1[\"neg\"] - sen2[\"neg\"])\n",
    "    diff_neu = np.abs(sen1[\"neu\"] - sen2[\"neu\"])\n",
    "    diff_pos = np.abs(sen1[\"pos\"] - sen2[\"pos\"])\n",
    "    diff_com = np.abs(sen1[\"compound\"] - sen2[\"compound\"])\n",
    "    return pd.Series([diff_neg, diff_neu, diff_pos, diff_com])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "sen_train = train_orig.apply(sentiment_analyze, axis = 1)\n",
    "sen_train.columns = [\"diff_sen_neg\", \"diff_sen_neu\", \"diff_sen_pos\", \"diff_sen_com\"]\n",
    "train = pd.concat([train, sen_train], axis = 1)\n",
    "\n",
    "sen_test = test_orig.apply(sentiment_analyze, axis = 1)\n",
    "sen_test.columns = [\"diff_sen_neg\", \"diff_sen_neu\", \"diff_sen_pos\", \"diff_sen_com\"]\n",
    "test = pd.concat([test, sen_test], axis = 1)\n",
    "\n",
    "del sen_train, sen_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.to_csv(\"train.csv\", index = False)\n",
    "test.to_csv(\"test.csv\", index = False)\n",
    "trainlabel.to_csv(\"trainlabel.csv\", index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
